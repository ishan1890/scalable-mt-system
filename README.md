âœ… Scalable Machine Translation System (Use Case 1)
ğŸ“Œ Project Overview

This project implements a scalable, priority-aware Machine Translation (MT) system designed to:

Handle urgent translation requests immediately

Scale automatically with traffic

Minimize GPU cost during idle periods

Provide monitoring, observability, and cost estimation

Be cloud-ready and production-deployable

This is a production-style prototype implemented on a local machine with a lightweight model, while keeping the full architecture ready for large GPU-based deployment.

âœ… What We Implemented (Fully Completed)
âœ… 1. Priority-Based Request Handling (P0â€“P3)

Every request contains a priority:

P0 â†’ Critical (highest priority)

P1 â†’ High

P2 â†’ Normal

P3 â†’ Low

Requests are placed into:

asyncio.PriorityQueue()


The worker always processes:

P0 â†’ P1 â†’ P2 â†’ P3


âœ… This guarantees urgent requests are processed immediately, even under heavy load.

âœ… 2. Asynchronous Background Worker

The system follows:

Client â†’ FastAPI â†’ Priority Queue â†’ Background Worker â†’ Model â†’ Response


API is non-blocking

Worker:

Pulls jobs from the queue

Runs translation

Sends results back via asyncio.Future

This ensures:

High concurrency

No API blocking

Easy horizontal scaling later

âœ… 3. Machine Translation Pipeline

Frameworks used:

FastAPI

Transformers

Torch

Model:

MarianMT (~300MB) for development

Translation happens inside:

translate_text()


âœ… Model can be swapped with a 60â€“100GB GPU model without changing architecture.

âœ… 4. Dockerized Deployment

The entire system runs in:

Docker container

Enables:

Cloud portability

Kubernetes deployment

Easy CI/CD integration

âœ… 5. Monitoring & Observability (Prometheus Ready)

Exposed metrics:

Total requests by priority

Queue waiting time

Processing latency

Active worker count

Accessed via:

GET /metrics


âœ… These metrics are directly usable for autoscaling and alerting.

âœ… 6. TFLOPS & Cost Estimation (Analytical)

Using these assumptions:

Parameter	Value
Server Compute	90 TFLOPS
Throughput	1000 words/min
GPU Cost	$2/hour

Daily estimates:

Load	Compute Cost	Always-On Cost
10k words/day	~$0.33	$48
100k words/day	~$3.33	$48
1M words/day	~$33.33	$48

âœ… This proves why autoscaling is critical for cost savings.

âŒ What We Did NOT Implement Fully (And WHY)
âŒ 1. 60â€“100GB Model Deployment
Why NOT implemented locally?

A 60â€“100GB MT model requires:

Data-center GPUs (A100 / H100)

40â€“80GB GPU VRAM

High RAM and disk bandwidth

Local machine limitations:
Limitation	Reality
GPU VRAM	Too small
System RAM	Insufficient
Disk IO	Too slow
Power & Cooling	Not suitable

âœ… We used a small 300MB model for development
âœ… Architecture remains identical for big models
âŒ Large model cannot physically run on this system

âŒ 2. Real GPU Autoscaling (Cloud Level)

True GPU autoscaling requires:

Kubernetes cluster

GPU node groups

Cluster Autoscaler / Karpenter

Local environment only provides:

One machine

One Docker host

No dynamic GPU creation/removal

âœ… We implemented:

Queue

Load metrics

Worker model
âŒ We did NOT deploy actual cloud autoscaling because real GPU clusters are required

âŒ 3. Real Cloud Billing Integration

True cost tracking requires:

AWS / GCP / Azure billing APIs

Production traffic

Real GPU usage data

âœ… We provided accurate analytical estimates
âŒ We did NOT connect real billing because this is not a cloud deployment

âœ… What We Would Do on a Big GPU Machine / Cloud

If this system is deployed on real infrastructure:

ğŸš€ 1. Swap Model with 60â€“100GB GPU Model

Load model on:

device = "cuda"


Use:

FP16 / BF16

TensorRT / DeepSpeed / vLLM

No architectural change required.

ğŸš€ 2. Deploy on Kubernetes with HPA

Deployment + Service

Horizontal Pod Autoscaler:

minReplicas: 1
maxReplicas: 20
scale on CPU + queue depth


GPU nodes auto-created by:

Cluster Autoscaler / Karpenter

âœ… This enables:

Zero GPU cost during idle

Instant scaling on traffic spikes

ğŸš€ 3. Priority Workers at Scale

Separate worker groups:

mt-worker-critical (P0)

mt-worker-normal (P2/P3)

P0 workers:

Higher pod priority

Higher minimum replicas

âœ… Guarantees SLA for urgent jobs

ğŸš€ 4. Real Cost & TFLOPS Tracking

Real GPU prices from cloud provider

Cost dashboards per:

100k words

1M words

Dynamic optimization of GPU spend

âœ… Code-Level Summary
Component	Purpose
FastAPI	API handling
PriorityQueue	Urgent job ordering
worker_loop()	Background execution
translate_text()	Model inference
Prometheus metrics	Monitoring
Docker	Packaging
Kubernetes (planned)	Autoscaling
âœ… Final Conclusion (Submission-Ready)

âœ… This project fully demonstrates:

Priority-based request handling

Scalable worker architecture

Containerized ML inference

Monitoring & performance visibility

Cost-aware system design

âŒ Full-scale 60â€“100GB GPU execution and autoscaling are not implemented locally due to hardware and environment limitations, not due to architectural constraints.

âœ… The system is cloud-ready and production-deployable without redesign.
